<div class="h5p-content h5p-initialized h5p-no-frame using-mouse" data-content-id="3071"><div class="h5p-container h5p-standalone h5p-interactive-book h5p-scrollable-fullscreen h5p-interactive-book-small"><div class="h5p-content-controls"><div role="button" tabindex="0" class="h5p-enable-fullscreen" aria-label="Fullscreen" title="Fullscreen"></div></div><div class="h5p-interactive-book-status-header" tabindex="-1"><div class="h5p-interactive-book-status-progressbar-back"><div class="h5p-interactive-book-status-progressbar-front" tabindex="-1" title="Page 5 of 5." style="width: 100%;"></div></div><div class="h5p-interactive-book-status"><button class="h5p-interactive-book-status-fullscreen h5p-interactive-book-status-button h5p-interactive-book-enter-fullscreen" title="Fullscreen" aria-label="Fullscreen"></button><button class="h5p-interactive-book-status-arrow h5p-interactive-book-status-button next" aria-label="Next page" disabled="disabled"><div class="navigation-button icon-next disabled" title="Next page"></div></button><button class="h5p-interactive-book-status-arrow h5p-interactive-book-status-button previous" aria-label="Previous page"><div class="navigation-button icon-previous" title="Previous page"></div></button><button class="h5p-interactive-book-status-menu h5p-interactive-book-status-button" aria-label="Toggle navigation menu" aria-expanded="false" aria-controls="h5p-interactive-book-navigation-menu"><a class="icon-menu"></a></button><div class="h5p-interactive-book-status-progress-wrapper"><p class="h5p-interactive-book-status-progress"><span class="h5p-interactive-book-status-progress-number" aria-hidden="true">5</span><span class="h5p-interactive-book-status-progress-divider" aria-hidden="true"> / </span><span class="h5p-interactive-book-status-progress-number" aria-hidden="true">5</span><p class="hidden-but-read">Page 5 of 5.</p></p></div><div class="h5p-interactive-book-status-chapter"><h1 class="title" title="Vision Transformers">Vision Transformers</h1></div></div></div><div class="h5p-interactive-book-main h5p-interactive-book-navigation-hidden"><div id="h5p-interactive-book-navigation-menu" class="h5p-interactive-book-navigation"><div class="h5p-interactive-book-navigation-maintitle"><h2 class="navigation-title" title="Transformer Networks">Transformer Networks</h2></div><ul class="navigation-list"><li class="h5p-interactive-book-navigation-chapter h5p-interactive-book-navigation-closed"><button tabindex="-1" class="h5p-interactive-book-navigation-chapter-button" aria-expanded="false" aria-controls="h5p-interactive-book-sectionlist-0"><div class="h5p-interactive-book-navigation-chapter-accordion icon-collapsed"></div><div class="h5p-interactive-book-navigation-chapter-title-text" title="Introduction to Transformers">Introduction to Transformers</div><div class="h5p-interactive-book-navigation-chapter-progress icon-chapter-done"></div></button><ul class="h5p-interactive-book-navigation-sectionlist" id="h5p-interactive-book-sectionlist-0" aria-hidden="true" tabindex="-1"><li class="h5p-interactive-book-navigation-section h5p-interactive-book-navigation-section-0"><button class="section-button" tabindex="-1"><div class="h5p-interactive-book-navigation-section-icon icon-chapter-blank"></div><div title="Introduction" class="h5p-interactive-book-navigation-section-title">Introduction</div></button></li><li class="h5p-interactive-book-navigation-section h5p-interactive-book-navigation-section-0"><button class="section-button" tabindex="-1"><div class="h5p-interactive-book-navigation-section-icon icon-chapter-blank"></div><div title="Historical Context" class="h5p-interactive-book-navigation-section-title">Historical Context</div></button></li></ul></li><li class="h5p-interactive-book-navigation-chapter h5p-interactive-book-navigation-closed"><button tabindex="-1" class="h5p-interactive-book-navigation-chapter-button" aria-expanded="false" aria-controls="h5p-interactive-book-sectionlist-1"><div class="h5p-interactive-book-navigation-chapter-accordion icon-collapsed"></div><div class="h5p-interactive-book-navigation-chapter-title-text" title="Core Concepts of Transformers">Core Concepts of Transformers</div><div class="icon-chapter-blank h5p-interactive-book-navigation-chapter-progress"></div></button><ul class="h5p-interactive-book-navigation-sectionlist" id="h5p-interactive-book-sectionlist-1" aria-hidden="true" tabindex="-1"><li class="h5p-interactive-book-navigation-section h5p-interactive-book-navigation-section-0"><button class="section-button" tabindex="-1"><div class="h5p-interactive-book-navigation-section-icon icon-chapter-blank"></div><div title="Self-attention Mechanism" class="h5p-interactive-book-navigation-section-title">Self-attention Mechanism</div></button></li><li class="h5p-interactive-book-navigation-section h5p-interactive-book-navigation-section-10"><button class="section-button" tabindex="-1"><div class="h5p-interactive-book-navigation-section-icon icon-chapter-blank"></div><div title="Multi-Head Attention" class="h5p-interactive-book-navigation-section-title">Multi-Head Attention</div></button></li><li class="h5p-interactive-book-navigation-section h5p-interactive-book-navigation-section-12"><button class="section-button" tabindex="-1"><div class="h5p-interactive-book-navigation-section-icon icon-chapter-blank"></div><div title="Encoder Block" class="h5p-interactive-book-navigation-section-title">Encoder Block</div></button></li><li class="h5p-interactive-book-navigation-section h5p-interactive-book-navigation-section-16"><button class="section-button" tabindex="-1"><div class="h5p-interactive-book-navigation-section-icon icon-chapter-blank"></div><div title="Positional Encoding" class="h5p-interactive-book-navigation-section-title">Positional Encoding</div></button></li></ul></li><li class="h5p-interactive-book-navigation-chapter h5p-interactive-book-navigation-closed"><button tabindex="-1" class="h5p-interactive-book-navigation-chapter-button" aria-expanded="false" aria-controls="h5p-interactive-book-sectionlist-2"><div class="h5p-interactive-book-navigation-chapter-accordion icon-collapsed"></div><div class="h5p-interactive-book-navigation-chapter-title-text" title="The Transformer Architecture">The Transformer Architecture</div><div class="icon-chapter-blank h5p-interactive-book-navigation-chapter-progress"></div></button><ul class="h5p-interactive-book-navigation-sectionlist" id="h5p-interactive-book-sectionlist-2" aria-hidden="true" tabindex="-1"><li class="h5p-interactive-book-navigation-section h5p-interactive-book-navigation-section-0"><button class="section-button" tabindex="-1"><div class="h5p-interactive-book-navigation-section-icon icon-chapter-blank"></div><div title="Overview" class="h5p-interactive-book-navigation-section-title">Overview</div></button></li><li class="h5p-interactive-book-navigation-section h5p-interactive-book-navigation-section-6"><button class="section-button" tabindex="-1"><div class="h5p-interactive-book-navigation-section-icon icon-chapter-blank"></div><div title="The Encoder WorkFlow" class="h5p-interactive-book-navigation-section-title">The Encoder WorkFlow</div></button></li><li class="h5p-interactive-book-navigation-section h5p-interactive-book-navigation-section-30"><button class="section-button" tabindex="-1"><div class="h5p-interactive-book-navigation-section-icon icon-chapter-blank"></div><div title="The Decoder WorkFlow" class="h5p-interactive-book-navigation-section-title">The Decoder WorkFlow</div></button></li></ul></li><li class="h5p-interactive-book-navigation-chapter h5p-interactive-book-navigation-closed"><button tabindex="-1" class="h5p-interactive-book-navigation-chapter-button" aria-expanded="false" aria-controls="h5p-interactive-book-sectionlist-3"><div class="h5p-interactive-book-navigation-chapter-accordion icon-collapsed"></div><div class="h5p-interactive-book-navigation-chapter-title-text" title="Real-Life Transformer Models">Real-Life Transformer Models</div><div class="icon-chapter-blank h5p-interactive-book-navigation-chapter-progress"></div></button><ul class="h5p-interactive-book-navigation-sectionlist" id="h5p-interactive-book-sectionlist-3" aria-hidden="true" tabindex="-1"><li class="h5p-interactive-book-navigation-section h5p-interactive-book-navigation-section-0"><button class="section-button" tabindex="-1"><div class="h5p-interactive-book-navigation-section-icon icon-chapter-blank"></div><div title="BERT" class="h5p-interactive-book-navigation-section-title">BERT</div></button></li><li class="h5p-interactive-book-navigation-section h5p-interactive-book-navigation-section-0"><button class="section-button" tabindex="-1"><div class="h5p-interactive-book-navigation-section-icon icon-chapter-blank"></div><div title="LaMDA" class="h5p-interactive-book-navigation-section-title">LaMDA</div></button></li><li class="h5p-interactive-book-navigation-section h5p-interactive-book-navigation-section-0"><button class="section-button" tabindex="-1"><div class="h5p-interactive-book-navigation-section-icon icon-chapter-blank"></div><div title="GPT and ChatGPT" class="h5p-interactive-book-navigation-section-title">GPT and ChatGPT</div></button></li></ul></li><li class="h5p-interactive-book-navigation-chapter"><button tabindex="0" class="h5p-interactive-book-navigation-chapter-button h5p-interactive-book-navigation-current" aria-expanded="true" aria-controls="h5p-interactive-book-sectionlist-4"><div class="h5p-interactive-book-navigation-chapter-accordion icon-expanded"></div><div class="h5p-interactive-book-navigation-chapter-title-text" title="Vision Transformers">Vision Transformers</div><div class="h5p-interactive-book-navigation-chapter-progress icon-chapter-done"></div></button><ul class="h5p-interactive-book-navigation-sectionlist" id="h5p-interactive-book-sectionlist-4"><li class="h5p-interactive-book-navigation-section h5p-interactive-book-navigation-section-0"><button class="section-button" tabindex="-1"><div class="h5p-interactive-book-navigation-section-icon icon-chapter-blank"></div><div title="Vision Transformers" class="h5p-interactive-book-navigation-section-title">Vision Transformers</div></button></li></ul></li></ul></div><div class="h5p-interactive-book-content" style="height: 1649px;"><div class="h5p-interactive-book-chapter h5p-column" id="h5p-interactive-book-chapter-d0ec552d-3e6d-4945-8df9-f217fb3e6955"><div><div class="h5p-column-content h5p-advanced-text" id="h5p-interactive-book-section-f1522558-5351-45ca-b7e3-290681a66d2b"><h2><strong>Introduction</strong></h2><p><span style="color:rgb(192,57,43)"><strong>Transformer Neural Network</strong></span>, or simply <span style="color:rgb(192,57,43)"><strong>Transformer</strong></span>, is a neural network architecture introduced in 2017 in the now-famous paper <a href="https://arxiv.org/abs/1706.03762">“Attention is all you need”</a>. The title refers to the attention mechanism, which forms the basis for data processing with Transformers.</p><p>Transformer Networks have been the predominant type of Deep Learning models for NLP in recent years. They replaced Recurrent Neural Networks in all NLP tasks, and also, all Large Language Models employ the Transformer Network architecture. As well as, Transformer Networks were recently adapted for other tasks and have outperformed other Machine Learning models for image processing and video processing tasks, protein and DNA sequence prediction, time-series data processing, and have been used for reinforcement learning tasks. Consequently, Transformers are currently the most important Neural Network architecture.</p><h2>Historical Context</h2><p>This pioneering concept was not just a theoretical advancement but also found practical implementation, notably in TensorFlow's <a href="https://github.com/tensorflow/tensor2tensor">Tensor2Tensor</a> package. Furthermore, the Harvard NLP group contributed to this burgeoning field by offering an <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html">annotated guide to the paper</a>, supplemented with a PyTorch implementation. You can learn more about <a href="https://www.datacamp.com/tutorial/building-a-transformer-with-py-torch">how to implement a Transformer from scratch</a> in our separate tutorial.</p><p>Their introduction has spurred a significant surge in the field, often referred to as Transformer AI. This revolutionary model laid the groundwork for subsequent breakthroughs in the realm of large language models, including BERT. By 2018, these developments were already being hailed as a watershed moment in NLP.</p><p>In 2020, researchers at OpenAI announced <a href="https://arxiv.org/abs/2005.14165">GPT-3.</a> Within weeks, GPT-3's versatility was quickly demonstrated when people were using it to create poems, programs, songs, websites and more captivating the imagination of users globally.</p><p>In a 2021 paper, Stanford scholars aptly termed these innovations <a href="https://www.datacamp.com/blog/what-are-foundation-models">foundation models</a>, underscoring their foundational role in reshaping AI. Their work highlights how transformer models have not only revolutionized the field but also pushed the frontiers of what's achievable in artificial intelligence, heralding a new era of possibilities.</p><p>“<em>We are in a time where simple methods like neural networks are giving us an explosion of new capabilities,</em>” Ashish Vaswani, an entrepreneur and former senior staff research scientist at Google</p></div></div></div><div class="h5p-interactive-book-chapter h5p-column" id="h5p-interactive-book-chapter-d49b118e-2e4d-4214-a039-92dc09bf2017"><div><div class="h5p-column-content h5p-advanced-text" id="h5p-interactive-book-section-c449e64b-19c4-4580-a8fb-e613e0cc3f02"><h2><strong>Self-attention Mechanism</strong></h2><p><span style="color:rgb(192,57,43)"><strong>Self-attention</strong></span> in NNs is a mechanism that forces a model to attend to portions of the data when making predictions. For instance, in NLP, self-attention mechanism is used to identify words in sentences that have significance for a given query word in the sentence. That is, the model should pay more attention to some words in sentences, and less attention to other words in sentences that are less relevant for a given task.</p><p>In the following two sentences, in the left subfigure the word “it” refers to “street”, while in the right subfigure the word “it” refers to “animal”. Understanding the relationships between the words in such sentences has been challenging with traditional NLP approaches. Transformers use the self-attention mechanism to model the relationships between all words in a sentence, and assign weights to other words in sentences based on their importance. In the left subfigure, the mechanism estimated that the<span style="color:rgb(192,57,43)"><strong> query word</strong></span> “it” is most related to the word “street”, but the word “it” is also somewhat related to the words “The” and “animal. These words are referred to as <span style="color:rgb(192,57,43)"><strong>key words</strong></span> for the query word “it”.The intensity of the lines connecting the words, as well as the intensity of the blue color, signifies the attention scores (i.e., weights). The wider and bluer the lines, the higher the attention scores between two words are.</p></div><div></div><div class="h5p-column-content h5p-image" id="h5p-interactive-book-section-6989cd72-bf88-40c4-8f4d-93e6092068f1"><img src="https://lms.fit.hanu.vn/pluginfile.php/1/core_h5p/content/3071/images/image-JvuSAo7A.png" alt="" title="" style="width: 100%; height: 100%;"></div><div></div><div class="h5p-column-content h5p-advanced-text" id="h5p-interactive-book-section-ac366021-497b-47bb-a76b-38ee70bfaf9a"><p><span>Specifically, Transformer Network compares each word to every other word in the sentence, and calculates attention scores. This is shown in the next figure, where for example, the word “caves” has the highest </span><span style="color:rgb(192,57,43)"><strong>attention scores</strong></span><span> for the words “glacier” and “formed”. The attention scores are calculated as the dot (i.e., inner) product of the input representations of two words. That is, for each Query word <em><strong>Q</strong></em> and Key word <em><strong>K</strong></em>, the attention score is <em><strong>Q.K</strong></em>.</span></p></div><div></div><div class="h5p-column-content h5p-image" id="h5p-interactive-book-section-476449f8-d15a-464c-9ad7-e786c174eaf0"><img src="https://lms.fit.hanu.vn/pluginfile.php/1/core_h5p/content/3071/images/image-y3cTNNie.png" alt="" title="" style="width: 100%; height: 100%;"></div><div></div><div class="h5p-column-content h5p-advanced-text" id="h5p-interactive-book-section-0f1b6342-9cc7-48b4-a5f4-1d147d6b50c0"><p>Transformers employ word embeddings for representing the individual words in text sequences (where each text sequence can have one or several sentences). Recall from the previous lecture that <span style="color:rgb(192,57,43)"><strong>word embeddings </strong></span>are vector representations of words, such that the vectors of words that have similar semantic meaning have close spatial positions in the embeddings space. Therefore, the attention scores are dot products of the embedding vectors for each pair of words in sentences.</p><p>The obtained attention scores for each word are then first scaled (by dividing the values by <em><strong>√d</strong></em> ) and afterward are normalized to be in the [0,1] range (by applying a softmax function). That is, the attention scores are calculated as:</p></div><div></div><div class="h5p-column-content h5p-image" id="h5p-interactive-book-section-8d15631a-1d00-4c2e-bc6b-5bc726ec415a"><img src="https://lms.fit.hanu.vn/pluginfile.php/1/core_h5p/content/3071/images/image-hYSNlXoT.png" alt="" title="" style="width: 100%; height: 100%;"></div><div></div><div class="h5p-column-content h5p-advanced-text" id="h5p-interactive-book-section-5388d696-e031-4b28-a09b-fc118518dce7"><p><span>where <em><strong>d</strong></em> is the dimensionality of the embedding vectors. Scaling the values by </span><em><strong>√d</strong></em><span> is helpful for improving the flow of the gradients during training. The resulting scaled and normalized attention scores are then multiplied with the initial representation of the words, which in the self-attention module is referred to as </span><span style="color:rgb(192,57,43)"><strong>value</strong></span><span> or <em><strong>V</strong></em>.</span></p></div><div class="h5p-column-ruler"></div><div class="h5p-column-content h5p-advanced-text" id="h5p-interactive-book-section-766fdeba-2ac4-4cc9-992a-ee6521f3b2b5"><p>This is shown in the next figure. The left subfigure shows the attention scores calculated as product of the input representations of the words <em><strong>Q</strong></em> and <em><strong>K</strong></em>, which are afterwards multiplied with the input representation <em><strong>V</strong></em> to obtain the output of the module. Note that for text classification, all three terms Query, Key, and Value are the same input representation of the words in sentences. However, the original Transformer was developed for machine translation, where the words in the target language are queries, and the words in the source language are pairs of keys and values. This terminology is also related to search engines, which compare queries to keys, and return values (e.g., the user submits a query, the search engine identifies key words within the query to search for, and it returns the results of the search as values). Self-attention works in a similar way, where each query word is matched to other key words, and a weighted value is returned.</p><p><span>The right subfigure below shows how self-attention is implemented in Transformer Networks. Namely, </span><span>Matmul </span><span>stands for a matrix multiplication layer which calculates the dot product <em><strong>Q.K</strong></em>, which is afterwards scaled by </span><em><strong>√d</strong></em><span>, then there is an optional masking layer, and afterward the final attention scores are obtained by passing it through a </span><span>Softmax</span><span> layer. Finally, the attention scores are multiplied with <em><strong>V</strong></em> via another matrix multiplication layer </span><span>Matmul</span><span> to calculate the output of the self-attention module. The optional masking layer can be used for two purposes: (a) to ensure that attention scores are not calculated for the padding tokens in padded sequences (e.g., 0 is often used as the padding token), but instead are calculated only for the positions in input sequences that have actual words in padded sequences; or (b) to set the attention scores for future tokens to zero, so that the model can only attend to previous tokens, as explained in the section below on decoder sub-networks).</span></p></div><div></div><div class="h5p-column-content h5p-image" id="h5p-interactive-book-section-324797fa-00f9-4130-9dc9-56f7424f6f2a"><img src="https://lms.fit.hanu.vn/pluginfile.php/1/core_h5p/content/3071/images/image-AZYPfvVN.png" alt="" title="" style="width: 100%; height: 100%;"></div><div></div><div class="h5p-column-content h5p-advanced-text" id="h5p-interactive-book-section-29f0635f-af80-4e9c-958a-4112712db1f4"><p>In conclusion, self-attention is applied to determine the meaning of the words in a sentence based on the context. That is, Transformers use the attention scores to modify the input vector representations for each word and generate a new representation based on the context of the sentence. During the training of the network, the representations of the words are updated and projected into a new embeddings space that takes the context into account.</p></div><div class="h5p-column-ruler"></div><div class="h5p-column-content h5p-advanced-text" id="h5p-interactive-book-section-c10659d9-c469-42ad-ab70-44f26cb22559"><h2><strong>Multi-Head Attention</strong></h2><p>Transformer Networks include multiple self-attention modules in their architecture. Each self-attention module is called <span style="color:rgb(192,57,43)"><strong>attention head</strong></span>, and the aggregation of the outputs of multiple attention heads is called <span style="color:rgb(192,57,43)"><strong>multi-head</strong></span><strong> </strong><span style="color:rgb(192,57,43)"><strong>attention</strong></span>. For instance, the original Transformer model had 8 attention heads, while the GPT-3 language model has 12 attention heads.</p><p>The multi-head attention module is shown in the next figure, where the inputs are first passed through a linear layer (dense or fully-connected layer), next they are fed to the multiple attention heads, and the outputs of all attention heads are concatenated, and passed through one more linear layer.</p><p>A logical question one can ask is why are multiple attention heads needed? The reason is that multiple attention modules can learn different relationships between the words in sentences. Each module can extract context independently from the other modules, which allows to capture less obvious context and enhance the learning capabilities of the model. For example, one head may capture relationship between the nouns and numerical values in sentences, another head may focus on the relationship between the adjectives in sentences, and another head may focus on rhyming words, etc. And, if one head becomes too specialized in capturing one type of patterns, the other heads can compensate for it and provide redundancy that can improve the overall performance of the model.</p><p>Also, the computations of each attention head can be performed in parallel on different workers, which allows for accelerating the training and scaling up the models.</p></div><div></div><div class="h5p-column-content h5p-image" id="h5p-interactive-book-section-5715f9ab-d98e-4645-97e5-717a2503fc92"><img src="https://lms.fit.hanu.vn/pluginfile.php/1/core_h5p/content/3071/images/image-c0CxlER5.png" alt="" title="" style="width: 100%; height: 100%;"></div><div></div><div class="h5p-column-content h5p-advanced-text" id="h5p-interactive-book-section-57840cb1-bc37-4e6f-ac41-cca0cca85294"><h2><strong>Encoder Block</strong></h2><p>The <span style="color:rgb(192,57,43)"><strong>Encoder Block</strong></span> in Transformer Networks is shown in the next figure. It processes the input word embeddings and extracts representations in text data that can afterwards be used for different NLP tasks.</p><p>The components in the Encoder Block are:</p><ul><li><em>Multi-head Attention layer</em>, which as explained, consists of multiple self-attention modules.</li><li><em>Dropout layer</em>, is a regular dropout layer.</li><li><em>Residual connections</em>, are skip connections in neural networks, where the input to a layer is added to the processed output of the layer. Residual connections were popularized in the ResNets models, as they were shown to stabilize the training and mitigate the problems of <em>vanishing and exploding gradients</em> in neural networks (i.e., they refer to cases when the gradients become too small or too large during training). In the figure, the <span>Add</span> term in the layer refers to the residual connection, which adds the input embeddings to the output of the Dropout layer.</li><li><em>Layer Normalization</em>, is an operation that is similar to the batch normalization in CNNs, but instead, it normalizes the outputs of each multi-head attention layer independently from the outputs of the other multi-head attention layers, and scales the data to have 0 mean and 1 standard deviation. This type of normalization is more adequate for text data. And, as we learned in the previous lectures, normalization improves the flow of gradients during training. The <span>Norm</span> term in the figure refers to the Layer Normalization operation.</li><li><em>Feed Forward network</em>, consists of 2 fully-connected (dense) layers that extract useful data representations.</li><li>The Encoder Block also contains one more <em>Dropout layer</em>, and another <em>Add &amp; Norm</em> layer that forms a residual connection for the input to the Feed Forward network and applies a layer normalization operation.</li></ul><p>Larger Transformer networks typically include several encoder blocks in a sequence. For instance, in the original paper the authors used 6 encoder blocks.</p></div><div></div><div class="h5p-column-content h5p-image" id="h5p-interactive-book-section-d25fbe77-1127-41af-93c7-e8051a49a386"><img src="https://lms.fit.hanu.vn/pluginfile.php/1/core_h5p/content/3071/images/image-crgvsBRJ.png" alt="" title="" style="width: 100%; height: 100%;"></div><div></div><div class="h5p-column-content h5p-advanced-text" id="h5p-interactive-book-section-0746f750-a9f6-40c8-a81e-256eff80b96b"><p>The implementation of the Encoder Block in Keras and TensorFlow is shown in the cell following the imported libraries.</p><p>The Encoder Block is implemented as a custom layer which is a subclass of the <span>Layer </span><span>class in Keras. The </span><span>__init__() </span><span>constructor method lists the definitions of the layers in the Encoder, and the method </span><span>call() </span><span>provides the forward pass with the flow of information through the layers.&nbsp;</span></p><ul><li><em>Multi-head attention</em> layer is implemented in Keras, and it can be directly imported. The arguments in the layer are: <span>num_heads</span> is the number of attention heads, and <span>key_dim</span> is the dimension of the embeddings of the input tokens.</li><li><em>Dropout</em> and <em>Normalization</em> layers are also directly imported, with arguments <span>rate</span> for the dropout rate, and <span>epsilon</span> is a small float added to the standard deviation to avoid division by 0.</li><li><em>Feed forward network</em> includes 2 dense layers, with the number of neurons set to <span>ff_dim</span> and <span>embed_dim</span>, respectively.</li></ul><p>The <span>call()</span> method specifies the forward pass of the network, and takes two parameters: <span>inputs</span> (the input embeddings to the network) and <span>training</span> (an argument which can be True or False). For the dropout layers, during the model training this argument is set to True and dropout is applied, while during inference the argument is set to False and dropout is not applied.</p><p>Each step in the <span>call()</span> method performs the data processing for one layer. Note that the <span>multi_head_attention</span> layer has as arguments the <span>inputs</span> twice, which is once for the key and once for the value in the self-attention. Also note the residual connections that are implemented in the layer normalization, e.g., the inputs are added to the output of the multi-head attention.</p></div><div class="h5p-column-ruler"></div><div class="h5p-column-content h5p-advanced-text" id="h5p-interactive-book-section-8178febb-b4fc-4a60-8d1a-edc757484f51"><pre><span><strong>import</strong></span><span style="font-size:0.75em"> </span><span><strong>tensorflow</strong></span><span style="font-size:0.75em"> </span><span><strong>as</strong></span><span style="font-size:0.75em"> </span><span><strong>tf</strong></span><span style="font-size:0.75em">
</span><span><strong>from</strong></span><span style="font-size:0.75em"> </span><span><strong>tensorflow</strong></span><span style="font-size:0.75em"> </span><span><strong>import</strong></span><span style="font-size:0.75em"> keras
</span><span><strong>from</strong></span><span style="font-size:0.75em"> </span><span><strong>keras.layers</strong></span><span style="font-size:0.75em"> </span><span><strong>import</strong></span><span style="font-size:0.75em"> MultiHeadAttention, LayerNormalization, Dropout, Dense, Embedding, Layer
</span><span><strong>from</strong></span><span style="font-size:0.75em"> </span><span><strong>keras</strong></span><span style="font-size:0.75em"> </span><span><strong>import</strong></span><span style="font-size:0.75em"> Sequential, Model</span></pre><pre><span><strong>class</strong></span><span style="font-size:0.75em"> </span><span><strong>TransformerEncoder</strong></span><span style="font-size:0.75em">(Layer):
    </span><span><strong>def</strong></span><span style="font-size:0.75em"> </span><span>__init__</span><span style="font-size:0.75em">(</span><span>self</span><span style="font-size:0.75em">, embed_dim, num_heads, ff_dim, rate</span><span>=0.1</span><span style="font-size:0.75em">):
        </span><span>super</span><span style="font-size:0.75em">()</span><span>.</span><span>__init__</span><span style="font-size:0.75em">()
        </span><span>self</span><span>.</span><span style="font-size:0.75em">multi_head_attention </span><span>=</span><span style="font-size:0.75em"> MultiHeadAttention(num_heads</span><span>=</span><span style="font-size:0.75em">num_heads, key_dim</span><span>=</span><span style="font-size:0.75em">embed_dim)
        </span><span>self</span><span>.</span><span style="font-size:0.75em">feed_forward_net </span><span>=</span><span style="font-size:0.75em"> Sequential([Dense(ff_dim, activation</span><span>=</span><span>"relu"</span><span style="font-size:0.75em">), Dense(embed_dim),])
        </span><span>self</span><span>.</span><span style="font-size:0.75em">layer_normalization1 </span><span>=</span><span style="font-size:0.75em"> LayerNormalization(epsilon</span><span>=1e-6</span><span style="font-size:0.75em">)
        </span><span>self</span><span>.</span><span style="font-size:0.75em">layer_normalization2 </span><span>=</span><span style="font-size:0.75em"> LayerNormalization(epsilon</span><span>=1e-6</span><span style="font-size:0.75em">)
        </span><span>self</span><span>.</span><span style="font-size:0.75em">dropout1 </span><span>=</span><span style="font-size:0.75em"> Dropout(rate)
        </span><span>self</span><span>.</span><span style="font-size:0.75em">dropout2 </span><span>=</span><span style="font-size:0.75em"> Dropout(rate)

    </span><span><strong>def</strong></span><span style="font-size:0.75em"> </span><span>call</span><span style="font-size:0.75em">(</span><span>self</span><span style="font-size:0.75em">, inputs, training):
        multi_head_att_output </span><span>=</span><span style="font-size:0.75em"> </span><span>self</span><span>.</span><span style="font-size:0.75em">multi_head_attention(inputs, inputs)
        multi_head_att_dropout </span><span>=</span><span style="font-size:0.75em"> </span><span>self</span><span>.</span><span style="font-size:0.75em">dropout1(multi_head_att_output, training</span><span>=</span><span style="font-size:0.75em">training)
        add_norm_output_1 </span><span>=</span><span style="font-size:0.75em"> </span><span>self</span><span>.</span><span style="font-size:0.75em">layer_normalization1(inputs </span><span>+</span><span style="font-size:0.75em"> multi_head_att_dropout)
        feed_forward_output </span><span>=</span><span style="font-size:0.75em"> </span><span>self</span><span>.</span><span style="font-size:0.75em">feed_forward_net(add_norm_output_1)
        feed_forward_dropout </span><span>=</span><span style="font-size:0.75em"> </span><span>self</span><span>.</span><span style="font-size:0.75em">dropout2(feed_forward_output, training</span><span>=</span><span style="font-size:0.75em">training)
        add_norm_output_2 </span><span>=</span><span style="font-size:0.75em"> </span><span>self</span><span>.</span><span style="font-size:0.75em">layer_normalization2(add_norm_output_1 </span><span>+</span><span style="font-size:0.75em"> feed_forward_dropout)
        </span><span><strong>return</strong></span><span style="font-size:0.75em"> add_norm_output_2</span></pre></div><div class="h5p-column-ruler"></div><div class="h5p-column-content h5p-advanced-text" id="h5p-interactive-book-section-751726b8-f8a5-4aa8-aa05-68bdb522981b"><h2><strong>Positional Encoding</strong></h2><p>We mentioned that Transformers use word embeddings as inputs, however, the embeddings alone don’t provide information about the order of words in sentences. Understandably, the order of the words in a sentence is important, and different order of the words can convey a different meaning. To provide such information, Transformer Network introduces <span style="color:rgb(192,57,43)"><strong>positional encoding</strong></span> for each word that is added to the input embedding, as shown in the next figure.</p></div><div></div><div class="h5p-column-content h5p-image" id="h5p-interactive-book-section-96b5c014-8a06-4983-a375-0e235ff785ab"><img src="https://lms.fit.hanu.vn/pluginfile.php/1/core_h5p/content/3071/images/image-35lH03OD.png" alt="" title="" style="width: 100%; height: 100%;"></div><div></div><div class="h5p-column-content h5p-advanced-text" id="h5p-interactive-book-section-c9d83cae-5350-4ce9-99cf-e6d1028d24d8"><p>There are different ways in which positional encoding can be implemented. In the original Transformer paper, the positional encoding is a vector that has the same size as the word embedding vector, and the authors used sine and cosine functions to create position vectors, which are afterwards scaled to be in the range from -1 to 1. Using such positional encoding, each encoding vector corresponds to a unique position in a sequence of words. This type is called <em>sinusoidal positional encoding</em>.</p><p>The following cell implements the addition of positional encoding to word embeddings in Keras. In this case, we will not use the approach for obtaining positional encodings based on sine and cosine functions, but instead we will use a simpler approach and learn the positional encodings in the same way the word embeddings are learned. This type of positional encoding is referred to as <em>learned positional encodings/embeddings</em>. Therefore, for both token and positional embeddings we will use the <span>Embedding</span> layer in Keras which we introduced in the previous lecture. The arguments in the <span>Embedding</span> layer are the input dimension <span>input_dim</span> and the dimension of the embedding vectors <span>output_dim</span>. For the token embeddings layer, the input dimension is the size of the vocabulary, whereas for the positional embeddings layer the input dimension is the length of the text sequences.</p><p>In the <span>call</span> method, first the length of the text sequences is assigned to <span>maxlen</span>. The function <span>tf.range</span> is similar to NumPy’s <span>linspace</span> and creates numbers in the range from <span>start</span> to <span>limit</span> with a step <span>delta</span>. Next, the two separate <span>Embedding</span> layers are called, and returned is the sum of the token and positional embeddings.</p></div><div class="h5p-column-ruler"></div><div class="h5p-column-content h5p-advanced-text" id="h5p-interactive-book-section-944e082e-3ecc-41e7-a60e-79e20b596d17"><pre><span><strong>class</strong></span><span style="font-size:0.75em"> </span><span><strong>TokenAndPositionEmbedding</strong></span><span style="font-size:0.75em">(Layer):
    </span><span><strong>def</strong></span><span style="font-size:0.75em"> </span><span>__init__</span><span style="font-size:0.75em">(</span><span>self</span><span style="font-size:0.75em">, maxlen, vocab_size, embed_dim):
        </span><span>super</span><span style="font-size:0.75em">()</span><span>.</span><span>__init__</span><span style="font-size:0.75em">()
        </span><span>self</span><span>.</span><span style="font-size:0.75em">token_embeddings </span><span>=</span><span style="font-size:0.75em"> Embedding(input_dim</span><span>=</span><span style="font-size:0.75em">vocab_size, output_dim</span><span>=</span><span style="font-size:0.75em">embed_dim)
        </span><span>self</span><span>.</span><span style="font-size:0.75em">positional_embeddings </span><span>=</span><span style="font-size:0.75em"> Embedding(input_dim</span><span>=</span><span style="font-size:0.75em">maxlen, output_dim</span><span>=</span><span style="font-size:0.75em">embed_dim)

    </span><span><strong>def</strong></span><span style="font-size:0.75em"> </span><span>call</span><span style="font-size:0.75em">(</span><span>self</span><span style="font-size:0.75em">, inputs):
        maxlen </span><span>=</span><span style="font-size:0.75em"> tf</span><span>.</span><span style="font-size:0.75em">shape(inputs)[</span><span>-1</span><span style="font-size:0.75em">]
        positions </span><span>=</span><span style="font-size:0.75em"> tf</span><span>.</span><span style="font-size:0.75em">range(start</span><span>=0</span><span style="font-size:0.75em">, limit</span><span>=</span><span style="font-size:0.75em">maxlen, delta</span><span>=1</span><span style="font-size:0.75em">)
        position_embeddings </span><span>=</span><span style="font-size:0.75em"> </span><span>self</span><span>.</span><span style="font-size:0.75em">positional_embeddings(positions)
        input_embeddings </span><span>=</span><span style="font-size:0.75em"> </span><span>self</span><span>.</span><span style="font-size:0.75em">token_embeddings(inputs)
        </span><span><strong>return</strong></span><span style="font-size:0.75em"> input_embeddings </span><span>+</span><span style="font-size:0.75em"> position_embeddings</span></pre></div></div></div><div class="h5p-interactive-book-chapter" id="h5p-interactive-book-chapter-bf8e5779-4e2a-4647-bec3-c3a2fa478146"></div><div class="h5p-interactive-book-chapter" id="h5p-interactive-book-chapter-c0ef96f7-cccc-4403-a91b-dd1985d22885"></div><div class="h5p-interactive-book-chapter h5p-column h5p-interactive-book-current" id="h5p-interactive-book-chapter-92604dcb-824d-41ca-a80b-f7e93e1c383f"><div><div class="h5p-column-content h5p-advanced-text" id="h5p-interactive-book-section-053f8e2c-80b0-4f55-afa3-09518c93ec0d"><h2><strong>Vision Transformers</strong></h2><p>After the initial success of Transformer Networks in NLP, recently they have been adapted for computer vision tasks as well. The initial Transformer model for vision tasks proposed in 2021 was called <span style="color:rgb(192,57,43)"><strong>Vision Transformer (ViT).</strong></span></p><p>The architecture of ViT is very similar to the Transformers used in NLP. However, Transformer Networks were designed for working with sequential data, while images are spatial data types. To consider each pixel in an image as a sequential token would be impractical and too time-consuming. Therefore, ViT splits images into a set of smaller image patches (16x16 pixels), and it uses the sequence of image patches as inputs to the model (i.e., each image patch is considered a token). Each image patch is first flattened to one-dimensional vector, and those vectors are afterward passed through a dense layer to learn lower-dimensional embeddings for each patch. Positional embeddings and class embeddings are added, and the sequences are fed to a standard transformer encoder. Class embeddings are vectors that correspond to different classes in the dataset. The encoder block in ViT is identical to the encoder in the original Transformer Network. The steps are depicted in the figure below.</p></div><div></div><div class="h5p-column-content h5p-image" id="h5p-interactive-book-section-19b42d45-e122-41c1-9eb7-b91a6f8534eb"><img src="https://lms.fit.hanu.vn/pluginfile.php/1/core_h5p/content/3071/images/image-gXKgAVT8.gif" alt="" title="" style="width: 100%; height: 100%;"></div><div></div><div class="h5p-column-content h5p-advanced-text" id="h5p-interactive-book-section-cf9f59d6-ab3b-4175-9e34-99f19cc93c12"><p>The authors trained 3 versions of ViT, called Base (12 encoder blocks, 768 embeddings dimension, 86M parameters), Large (24 encoder blocks, 1,024 embeddings dimension, 307M parameters), and Huge (32 encoder blocks, 1,280 embeddings dimension, 632M parameters).</p><p>Various other versions of vision transformers were introduced recently, which include MaxViT (Multi-axis ViT), Swin (Shifted Window ViT), DeiT (Data-efficient image Transformer), T2T-ViT (Token-to-token ViT), and others. These models achieved higher accuracy on many vision tasks in comparison to Convolutional Neural Networks (EffNet, ConvNeXt, NFNet). The following figure shows the accuracy on ImageNet.</p></div><div></div><div class="h5p-column-content h5p-image" id="h5p-interactive-book-section-27b2f1b4-5257-4098-b912-43f9c090235f"><img src="https://lms.fit.hanu.vn/pluginfile.php/1/core_h5p/content/3071/images/image-1oa7fCEP.png" alt="" title="" style="width: 100%; height: 100%;"></div></div></div></div></div><div class="h5p-interactive-book-status-footer" tabindex="-1"><div class="h5p-interactive-book-status-progressbar-back"><div class="h5p-interactive-book-status-progressbar-front" tabindex="-1" title="Page 5 of 5." style="width: 100%;"></div></div><div class="h5p-interactive-book-status"><button class="h5p-interactive-book-status-to-top h5p-interactive-book-status-button h5p-interactive-book-status-arrow" aria-label="Navigate to the top"><div class="icon-up navigation-button"></div></button><button class="h5p-interactive-book-status-arrow h5p-interactive-book-status-button next" aria-label="Next page" disabled="disabled"><div class="navigation-button icon-next disabled" title="Next page"></div></button><button class="h5p-interactive-book-status-arrow h5p-interactive-book-status-button previous" aria-label="Previous page"><div class="navigation-button icon-previous" title="Previous page"></div></button><div class="h5p-interactive-book-status-progress-wrapper"><p class="h5p-interactive-book-status-progress"><span class="h5p-interactive-book-status-progress-number" aria-hidden="true">5</span><span class="h5p-interactive-book-status-progress-divider" aria-hidden="true"> / </span><span class="h5p-interactive-book-status-progress-number" aria-hidden="true">5</span><p class="hidden-but-read">Page 5 of 5.</p></p></div><div class="h5p-interactive-book-status-chapter"><h1 class="title" title="Vision Transformers">Vision Transformers</h1></div></div></div></div></div>